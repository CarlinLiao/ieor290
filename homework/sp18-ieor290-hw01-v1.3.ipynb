{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW01\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Throughout the semester, most of the homeworks will be a blend between mathematical analysis and coding implementations. \n",
    "\n",
    "To run this homework, as well as future homeworks, you will need to have Python 3, Jupyter notebook, NumPy, and a few other packages. The easiest way to get this installed and setup on your computer is to install Anaconda, which will automatically install everything you need. This can be found at https://www.anaconda.com/download/. Additionally, homeworks will require some knowledge of Markdown notation in Jupyter notebook.\n",
    "\n",
    "In addition to the technical content of this homework, the goal of this assignment is to familiarize you with Python, NumPy, Jupyter notebooks, Markdown notation and the LaTeX commands used within, as well as the Gradescope interface for homework submissions.\n",
    "\n",
    "If you have any questions about installing Anaconda or running the Jupyter notebooks, or about Markdown notation and the Gradescope interface, please post the questions on Piazza. For convenience, here is a link to this course's Piazza: https://piazza.com/class/jc6motxzn7s1qy.\n",
    "\n",
    "First, we're going to implement the evaluation of a cost function. Then, we will calculate the gradient analytically. From this point on, we will implement the calculation of the gradient at a point, and verify this with a finite differences approximation. Next, we will implement gradient descent, and finally, we will calculate necessary optimality conditions for these problems analytically. This will be done for two common cost functions.\n",
    "\n",
    "## Linear regression\n",
    "\n",
    "Suppose we're given data points $(x_i,y_i)$ for $i = 1,\\dots,N$. Here, $x_i \\in \\mathbb{R}^n$ and $y_i \\in \\mathbb{R}$.\n",
    "\n",
    "We want to form a linear approximation for $y_i$ given $x_i$, so we'll take functions of the form $f(x;\\theta) = \\theta^T x$. We'll suppose that the classification cost for each data point is:\n",
    "$$\n",
    "\\ell(x_i,y_i;\\theta) = \n",
    "\\frac{1}{2} \\| f(x_i;\\theta) - y_i \\|_2^2 = \n",
    "\\frac{1}{2} \\| \\theta^T x_i - y_i\\|_2^2\n",
    "$$\n",
    "\n",
    "Thus, the cost for the classification error across the whole dataset is given by:\n",
    "$$\n",
    "\\sum_{i = 1}^N \\ell(x_i,y_i;\\theta) = \n",
    "\\sum_{i = 1}^N \\frac{1}{2} \\| \\theta^T x_i - y_i\\|_2^2\n",
    "$$\n",
    "\n",
    "Let's define a matrix $X$ such that the $i$th row of $X$ is $x_i^\\top$. Thus, $X$ is a $N \\times n$ matrix. Similarly, define a vector $Y$ where the $i$th entry is $y_i$, so $Y$ is an $N \\times 1$ vector. Then the classification error can be written:\n",
    "$$\n",
    "\\ell(X,Y;\\theta) =\n",
    "\\sum_{i = 1}^N \\ell(x_i,y_i;\\theta) = \n",
    "\\frac{1}{2} \\| X \\theta - Y \\|_2^2\n",
    "$$\n",
    "This is often referred to as the *empirical risk*.\n",
    "\n",
    "When we are given the dataset, each particular $\\theta$ incurs some cost. First, write code to evaluate the empirical risk for the provided dataset.\n",
    "\n",
    "### Cost evaluation implementation\n",
    "\n",
    "You will be implementing a function which evaluates the empirical risk. \n",
    "\n",
    "First, run the '`Load packages.`' cell to load packages. The code has already been written for you. (This only needs to be done when you first open the Jupyter notebook shell.) \n",
    "\n",
    "Next, run the '`Generate dummy data.`' cell to generate dummy data. Time-permitting, it's always best to simulate data when first implementing and testing machine learning algorithms. Afterward, real data can be fed in and one can separate bugs in the implementation of the algorithm from quirks of the dataset more easily.\n",
    "\n",
    "The '`Function definition.`' cell for the function definition has the function interface defined, but needs to be implemented.\n",
    "\n",
    "The '`Call the linear regression cost evaluation function.`' cell evaluates your function on a test point and outputs the value. Run this after running the cell defining the function definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages.\n",
    "\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data.\n",
    "\n",
    "N = 50\n",
    "n = 5\n",
    "\n",
    "np.random.seed(15)\n",
    "\n",
    "X = np.random.rand(N,n)\n",
    "true_theta = np.random.rand(n)\n",
    "Y = np.dot( X, true_theta ) + (np.random.randn(N) / 100)\n",
    "\n",
    "random_theta = np.random.rand(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Complete the function implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition.\n",
    "\n",
    "def linear_regression_cost(X,Y,theta):\n",
    "    \n",
    "    return np.linalg.norm(X@theta - Y)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at true theta:\n",
      "0.0334452721755\n",
      "cost at a random theta:\n",
      "1.0882985358\n"
     ]
    }
   ],
   "source": [
    "# Call the linear regression cost evaluation function.\n",
    "\n",
    "print('cost at true theta:')\n",
    "print(linear_regression_cost(X,Y,true_theta))\n",
    "\n",
    "print('cost at a random theta:')\n",
    "print(linear_regression_cost(X,Y,random_theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient calculation\n",
    "\n",
    "Now, let's calculate the gradient. Please type in the gradient of $\\ell(X,Y;\\theta)$ with respect to $\\theta$ in the Markdown cell below. Note that $X$ and $Y$ are the data, which is seen as fixed in this context.\n",
    "\n",
    "The gradient must be typed in LaTeX notation. LaTeX is a typesetting language which is standard for usage in mathematical typesetting. If you have not used LaTeX before, this Jupyter Notebook itself has most of the notation required for this problem, so double-click any Markdown cell to see the syntax that generates different equations. Then, Run the Markdown cell to format it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Calculate the gradient\n",
    "\n",
    "Modify the LaTeX equation below to express the gradient of the loss function with respect to parameters $\\theta$. (You can edit this Markdown cell by double clicking.)\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\ell(X,Y;\\theta) = 2 \\theta^T \\theta X - 2 \\theta^T Y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient implementation\n",
    "\n",
    "Now that you've explicitly calculated an expression for the gradient, let's write a function that evaluates this explicit expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Implement the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition. \n",
    "\n",
    "def linear_regression_gradient(X,Y,theta):\n",
    "        \n",
    "    return X.T@X@theta - Y.T@X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient at true theta:\n",
      "[-0.00534415  0.00533884  0.00657914 -0.01886039 -0.04082041]\n",
      "gradient at a random theta:\n",
      "[ 1.63013637  1.83725472  2.57619965  4.72305271 -0.14001314]\n"
     ]
    }
   ],
   "source": [
    "# Call the linear regression gradient function.\n",
    "\n",
    "print('gradient at true theta:')\n",
    "print(linear_regression_gradient(X,Y,true_theta))\n",
    "\n",
    "print('gradient at a random theta:')\n",
    "print(linear_regression_gradient(X,Y,random_theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finite differences approximations of the gradient\n",
    "\n",
    "For more general machine learning problems, it's often difficult to analytically calculate the gradient. (Linear regression is one of the easiest in this regard.) In these settings, a common trick is to use a finite-difference approximation of the gradient.\n",
    "\n",
    "Additionally, even when the gradient can be calculated, finite difference implementations can help validate that there were no errors in implementation.\n",
    "\n",
    "Recall that the gradient $\\nabla f(x)$ has $\\frac{\\partial}{\\partial x_i} f(x)$ as its $i$th element. By definition:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_i} f(x) = \\lim_{h \\downarrow 0} \\frac{f(x + h e_i) - f(x)}{h}\n",
    "$$\n",
    "Recall that $e_i$ is the vector which has 1 in the $i$th coordinate and 0 everywhere else. So, if we do this for $i = 1,\\dots,n$, we can approximate each element of the gradient vector.\n",
    "\n",
    "There are a few methods to estimate finite differences, but usually the centered difference method is best. We will implement this finite-differences method now.\n",
    "So, we take $h$ very small and calculate:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_i} f(x) \\approx \\frac{f(x + \\frac{1}{2}h e_i) - f(x - \\frac{1}{2}h e_i)}{h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Implement the finite differences gradient approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition. \n",
    "\n",
    "def finite_differences(f, x, h):\n",
    "    \n",
    "    return (f(x+h*np.ones(x.shape)/2)-f(x-h*np.ones(x.shape)/2))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic gradient at true theta:\n",
      "[-0.00534415  0.00533884  0.00657914 -0.01886039 -0.04082041]\n",
      "finite difference approximation at true theta:\n",
      "-0.393593444872\n",
      "analytic gradient at random theta:\n",
      "[ 1.63013637  1.83725472  2.57619965  4.72305271 -0.14001314]\n",
      "finite difference approximation at random theta:\n",
      "2.44109268283\n"
     ]
    }
   ],
   "source": [
    "# Compare the finite differences method with the analytic gradient.\n",
    "\n",
    "print('analytic gradient at true theta:')\n",
    "print(linear_regression_gradient(X,Y,true_theta))\n",
    "\n",
    "print('finite difference approximation at true theta:')\n",
    "print(finite_differences((lambda theta : linear_regression_cost(X,Y,theta)), true_theta, 1e-3))\n",
    "\n",
    "print('analytic gradient at random theta:')\n",
    "print(linear_regression_gradient(X,Y,random_theta))\n",
    "\n",
    "print('finite difference approximation at random theta:')\n",
    "print(finite_differences((lambda theta : linear_regression_cost(X,Y,theta)), random_theta, 1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement gradient descent\n",
    "\n",
    "In class, we discussed gradient descent algorithms. Let us implement the gradient update now. Recall that the gradient step is simply:\n",
    "$$\n",
    "x^+ = x - \\alpha \\nabla f(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Implement gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition.\n",
    "\n",
    "def gradient_step( x, gradient, alpha ):\n",
    "    \n",
    "    return x - alpha*gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial theta:\n",
      "[ 0.83041891  0.2196513   0.43810492  0.88428638  0.04987925]\n",
      "final theta:\n",
      "[ 0.92450041  0.25729156  0.0781832   0.05390196  0.81262707]\n",
      "true theta:\n",
      "[ 0.92547864  0.26051745  0.08258404  0.05352811  0.80512636]\n",
      "cost at true theta:\n",
      "0.0334452721755\n",
      "cost at gradient descent theta:\n",
      "0.0320942329426\n"
     ]
    }
   ],
   "source": [
    "# Use the gradient_step function to iterate several times and perform gradient descent.\n",
    "\n",
    "theta_s = random_theta\n",
    "print('initial theta:')\n",
    "print(theta_s)\n",
    "\n",
    "for t in range(500):\n",
    "    theta_s = gradient_step(theta_s, linear_regression_gradient(X,Y,theta_s),0.01)\n",
    "\n",
    "print('final theta:')\n",
    "print(theta_s)\n",
    "\n",
    "print('true theta:')\n",
    "print(true_theta)\n",
    "\n",
    "print('cost at true theta:')\n",
    "print(linear_regression_cost(X,Y,true_theta))\n",
    "\n",
    "print('cost at gradient descent theta:')\n",
    "print(linear_regression_cost(X,Y,theta_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "\n",
    "### Calculate necessary optimality conditions\n",
    "\n",
    "Recall that the first-order necessary condition for optimality is that $\\nabla f(x^{opt}) = 0$. \n",
    "\n",
    "In our previous example, suppose $X^\\top X$ is invertible. What is the optimal $\\theta$ then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Calculate necessary condition\n",
    "\n",
    "Modify the LaTeX equation below to express the first-order necessary condition for optimality. (You can edit this Markdown cell by double clicking.)\n",
    "\n",
    "The necessary condition becomes:\n",
    "$$\n",
    "\\nabla f(x) = \\vec{0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement first-order condition to find optimum\n",
    "\n",
    "We've found an expression for the optimal $\\theta$, so now implement this in code and compare it with the results from gradient descent.\n",
    "\n",
    "Note that inverting a matrix is frequently a very expensive operation, and, in practice, it is often computationally cheaper to left-divide or right-divide by a matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Implement necessary condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_linear_regression(X,Y):\n",
    "    \n",
    "    return np.linalg.inv(X.T@X)@X.T@Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at true theta:\n",
      "0.0334452721755\n",
      "cost at gradient descent theta:\n",
      "0.0320942329426\n",
      "optimal theta:\n",
      "[ 0.92450044  0.25729161  0.07818307  0.05390184  0.81262722]\n",
      "cost at optimal theta:\n",
      "0.0320942329419\n"
     ]
    }
   ],
   "source": [
    "print('cost at true theta:')\n",
    "print(linear_regression_cost(X,Y,true_theta))\n",
    "\n",
    "print('cost at gradient descent theta:')\n",
    "print(linear_regression_cost(X,Y,theta_s))\n",
    "\n",
    "opt_theta = solve_linear_regression(X,Y)\n",
    "print('optimal theta:')\n",
    "print(opt_theta)\n",
    "\n",
    "print('cost at optimal theta:')\n",
    "print(linear_regression_cost(X,Y,opt_theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Submit your HW\n",
    "\n",
    "Now that you've finished, please click \"File -> Download as -> PDF via LaTeX\", and submit the PDF file on Gradescope. The Gradescope for this course can be found at https://gradescope.com/courses/14561. Homeworks will be collected through Gradescope throughout this semester, so take this opportunity to familiarize yourself with the interface."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

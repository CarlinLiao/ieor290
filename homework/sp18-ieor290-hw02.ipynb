{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "## Monday, February 5, 2018\n",
    "\n",
    "Today's class focuses on Support Vector Machines (SVMs), a method of doing maximum margin classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "\n",
    "#Generate dummy data and instantiate functions\n",
    "#Run this cell to initialize homework\n",
    "\n",
    "np.random.seed(10)\n",
    "N=100\n",
    "\n",
    "slope=np.random.uniform(-10,10)\n",
    "bias=np.random.choice([np.random.uniform(-100,-90),np.random.uniform(90,100)])\n",
    "\n",
    "#Define linear data\n",
    "data=np.zeros((N,2))\n",
    "data[:,0]=np.random.uniform(-20,20,N)\n",
    "data[:,1]=slope*data[:,0]+np.random.normal(0,10,N)\n",
    "data[int(N/2):,1]=data[int(N/2):,1]+bias\n",
    "\n",
    "#Define labels for data\n",
    "labels=np.ones(N)\n",
    "labels[int(N/2):]=-1\n",
    "\n",
    "#Define circular data\n",
    "r1=np.random.uniform(1,5)\n",
    "r2=np.random.uniform(7,10)\n",
    "theta=np.random.uniform(0,2*np.pi,N)\n",
    "np.random.normal(0,2,int(N/2))\n",
    "data2=np.zeros((N,2))\n",
    "data2[:int(N/2),:]=np.array([r1*np.cos(theta[:int(N/2)])+np.random.normal(0,0.5,int(N/2)),r1*np.sin(theta[:int(N/2)])+np.random.normal(0,0.5,int(N/2))]).T\n",
    "data2[int(N/2):,:]=np.array([r2*np.cos(theta[int(N/2):])+np.random.normal(0,0.5,int(N/2)),r2*np.sin(theta[int(N/2):])+np.random.normal(0,0.5,int(N/2))]).T\n",
    "\n",
    "\n",
    "#Classifier plotting function\n",
    "def plot2DClassifier(B,B0):\n",
    "    \n",
    "    x1=np.linspace(-20,20,1000)\n",
    "    x2=-1*(B[0]*x1+B0)/B[1]\n",
    "    plt.plot(x1,x2,'k',linewidth=4)\n",
    "    return\n",
    "\n",
    "def plot1DClassifier(B,B0):\n",
    "    \n",
    "    x=-B0/B\n",
    "    plt.plot(x,0,'|',ms=50,c='k')\n",
    "    \n",
    "    return\n",
    "\n",
    "def plotData(X):\n",
    "    plt.plot(X[:int(N/2),0],X[:int(N/2),1],'ro')\n",
    "    plt.plot(X[int(N/2):,0],X[int(N/2):,1],'bo')\n",
    "    return\n",
    "\n",
    "def plotKernelData(phi,X):\n",
    "    \n",
    "    transformed_data=np.zeros(N)\n",
    "    for i in range(N):\n",
    "        transformed_data[i]=phi(X[i,:])\n",
    "    \n",
    "    plt.plot(transformed_data[:int(N/2)],np.zeros(int(N/2)),'ro')\n",
    "    plt.plot(transformed_data[int(N/2):],np.zeros(int(N/2)),'bo')\n",
    "    return\n",
    "\n",
    "#Optimizing function\n",
    "def optimize(func,constraint,X,Y):\n",
    "    \n",
    "    print ('Optimizing...(May take some time to run)')\n",
    "    bound=[(0,None)]*N\n",
    "    objective=lambda x: func(x,X,Y)\n",
    "    equality_constraint=lambda x: constraint(x,X,Y)\n",
    "    \n",
    "    lagrange=opt.minimize(objective,x0=np.ones(N),bounds=bound,constraints={'type': 'eq', 'fun': equality_constraint})\n",
    "\n",
    "    return lagrange.x\n",
    "\n",
    "#Optimizing function\n",
    "def plotSupportVectors(svs,X):\n",
    "    \n",
    "    for i in range(len(svs)):\n",
    "        if svs[i]:\n",
    "            plt.plot(X[i,0],X[i,1],'y*',ms=10)\n",
    "    \n",
    "    return\n",
    "\n",
    "def plotKernelSVM(b):\n",
    "    \n",
    "    theta=np.linspace(0,2*np.pi,1000)\n",
    "    plt.plot(b*np.cos(theta),b*np.sin(theta),'k')\n",
    "    \n",
    "    return\n",
    "\n",
    "def transformData(phi,X):\n",
    "    phiX=np.zeros((N,1))\n",
    "    for i in range(N):\n",
    "        phiX[i,0]=phi(X[i,:])\n",
    "    return phiX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Suppose we're given data points $x_i \\in \\mathbb{R}^n$ and $y_i \\in \\{-1,+1\\}$, for $i = 1,\\dots,N$. \n",
    "\n",
    "The $x_i$ are the features and $y_i$ are labels taking values in either $-1$ or $+1$. The $x_i$ could be the pixel intensities in an image of an animal, and $y_i$ could indicate whether the animal is a `cat` or ` dog`, for example. For machine learning, we want to get a good guess of what $y_i$ is from only looking at $x_i$; so, the computer algorithm can take in images and distinguish between cats and dogs. \n",
    "\n",
    "## Defining a linear classifier\n",
    "\n",
    "Our dataset is then $(x_1,y_1),\\dots,(x_N,y_N)$.\n",
    "\n",
    "Now let's fix some parameters $\\beta \\in \\mathbb{R}^n$ and $\\beta_0 \\in \\mathbb{R}$. We can define a hyperplane as:\n",
    "\n",
    "$$\\{ x : x^{T} \\beta + \\beta_0 = 0 \\}$$\n",
    "\n",
    "This cuts our feature space, $\\mathbb{R}^n$, in half. That is, every point in $\\mathbb{R}^n$ is either above, below, or on this hyperplane. \n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "f(x;\\beta,\\beta_0) = x^T \\beta + \\beta_0\n",
    "$$\n",
    "\n",
    "Sometimes, when it is obvious, we'll drop the dependence on $\\beta,\\beta_0$ and just denote this $f(x)$.\n",
    "\n",
    "We can define a classifier:\n",
    "\n",
    "$$\n",
    "G(x) = \n",
    "\\begin{cases}\n",
    "+1 & \\text{ if } f(x) \\geq 0 \\\\\n",
    "-1 & \\text{ if } f(x) < 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Linear Classifier\n",
    "\n",
    "We begin by creating a function that takes in $\\beta$ and $\\beta_0$ and outputs a label for each datapoint. Note that $X \\in \\mathbb{R}^{N \\times n}$ is a matrix of $N$ datapoints. In this example, $n = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(beta,beta_0,X):\n",
    "         \n",
    "        Yhat=np.zeros(len(X))\n",
    "        \n",
    "        # TODO\n",
    "        \n",
    "        return Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to test a classifier with a random linear separator\n",
    "\n",
    "test_beta=np.array([slope/np.random.uniform(1,10),-1])\n",
    "test_beta_0=bias/3.0\n",
    "\n",
    "print()\n",
    "print()\n",
    "print (str(np.sum(classify(test_beta,test_beta_0,data)==1)) + ' Data points were classified as +1')\n",
    "print (str(np.sum(classify(test_beta,test_beta_0,data)==-1)) + ' Data points were classified as -1')\n",
    "print (str(np.sum(classify(test_beta,test_beta_0,data)==labels))+ ' Data points were classified correctly')\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plotData(data)\n",
    "plot2DClassifier(test_beta,test_beta_0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Margin Classification\n",
    "\n",
    "Having defined a general function that classifies points, we now try to find the maximum margin classifier.\n",
    "\n",
    "Suppose our dataset is strongly separable. (Sanity check: Is the data we are using separable by a hyperplane?). There are infinitely many hyperplanes that can separate this data. Which one of these separators is 'best'? The answer is the one with the largest margin.\n",
    "\n",
    "Recall from class that the problem that maximum margin classification is trying to solve is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\beta,\\beta_0}~ & \\|\\beta\\|^2_2 \\\\\n",
    "\\text{subject to }~\t\t& y_i (x_i^T \\beta + \\beta_0) \\geq 1 \\text{ for } i = 1,\\dots,N \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note that this problem has an inequality constraint for every datapoint. This can be hard to deal with when that amount of data we are using gets very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Maximum Margin Classifification through duality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to solve this problem by formulating the dual problem. First derive lagrangian of the minimization above. \n",
    "\n",
    "$$\n",
    "L(\\beta,\\beta_0,\\mu)= ???\n",
    "$$\n",
    "\n",
    "(Note that for some portion of the class, we separated our Lagrange multiplier into an $\\alpha$ and a $\\mu$ component. For the purposes of this homework, we will keep the Lagrange multipliers unified with the single vector $\\mu$. This will make coding easier.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the optimization problem is now:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\beta,\\beta_0} \\max_{\\mu\\ge0}~ &  L(\\beta,\\beta_0,\\mu)\n",
    "\\end{align*}\n",
    "\n",
    "Due to the convexity of the problem, we can rewrite this as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\mu\\ge0} \\min_{\\beta,\\beta_0}~ &  L(\\beta,\\beta_0,\\mu)\n",
    "\\end{align*}\n",
    "\n",
    "This equivalence is often referred to as **strong duality**.\n",
    "\n",
    "By solving the now unconstrained minimization:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta,\\beta_0} \\ \\ L(\\beta,\\beta_0,\\mu)\n",
    "$$\n",
    "\n",
    "Formulate a problem in the form:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\mu \\ge 0}~ & f(\\mu) \\\\\n",
    "\\text{subject to }~\t& g(\\mu)=0 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note that in this dual form, you only have 1 equality constraint.\n",
    "(Hint: Solve for the optimality conditions of $\\beta$ and $\\beta_0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Dual problem formulation\n",
    "\n",
    "Note you have access to the data $x_1,...x_N$ and the labels $y_1,...y_N$.\n",
    "\n",
    "$$\n",
    "f(\\mu)=???\n",
    "$$\n",
    "\n",
    "$$\n",
    "g(\\mu)=???\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the dual problem formulation above, we will now use the scikit learn python library to solve the dual, and thus the primal problem. \n",
    "\n",
    "# Task 4: code the dual problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out functions h, and g above, taking in the vector of lagrange multipliers mu\n",
    "# Once you have written the functions, we will call scipy-optimize (this is done for you).\n",
    "\n",
    "def f(mu,X,Y):\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def g(mu,X,Y):\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given the Lagrange multipliers, find $\\beta$ and $\\beta_0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take in the vector of lagrange multipliers and calculate B, and B_0\n",
    "\n",
    "def lagrange_to_classifier(mu,X,Y):\n",
    "    \n",
    "    beta=0\n",
    "    for i in range(len(mu)):\n",
    "        beta+=mu[i]*X[i,:]*Y[i]\n",
    "    \n",
    "    for i in range(len(mu)):\n",
    "        if mu[i]==np.max(mu):\n",
    "            beta_0=Y[i]-np.dot(beta,X[i,:])\n",
    "    \n",
    "    return beta, beta_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final SVM\n",
    "\n",
    "Now you can run the cell below to see the results of your coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the results\n",
    "\n",
    "lagrange_multipliers=optimize(f,g,data,labels)\n",
    "beta,beta_0=lagrange_to_classifier(lagrange_multipliers,data,labels)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print (str(np.sum(classify(beta,beta_0,data)==1)) + ' Data points were classified as +1')\n",
    "print (str(np.sum(classify(beta,beta_0,data)==-1)) + ' Data points were classified as -1')\n",
    "print (str(np.sum(classify(test_beta,test_beta_0,data)==labels))+ ' Data points were classified correctly')\n",
    "print()\n",
    "print()\n",
    "\n",
    "plt.figure()\n",
    "plot2DClassifier(beta,beta_0) \n",
    "plotData(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data points that determine the maximum margin are commonly referred to as 'support vectors'. These are the data points that determine the separating hyperplane. From the lagrange multipliers you found above, find the support vectors of your classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Finding the support vectors\n",
    "\n",
    "Given the Lagrange multipliers you found previously, how can you find the support vectors of your classifier? Write out how you propose to find the support vectors of your classifier from the lagrange multipliers below:\n",
    "\n",
    "## Task 5 written response:\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: implement Task 5\n",
    "\n",
    "Given the Lagrange multipliers $\\mu \\in \\mathbb{R}^N$, and your data points $X\\in \\mathbb{R}^{N \\times n}$ where $n=2$, write code that returns a length $N$ numpy array with $1$ if data point $X_i=X[i,:]$ is a support vector, and $0$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSupportVectors(mu,X):\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return [False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vectors=findSupportVectors(np.round(lagrange_multipliers,10),data)\n",
    "\n",
    "plt.figure()\n",
    "plot2DClassifier(beta,beta_0) \n",
    "plotData(data)\n",
    "plotSupportVectors(support_vectors,data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel SVM\n",
    "\n",
    "Most of the time the data points are not linearly separable as they are. In the next section we are going to learn about Kernel SVM. The high-level idea is to 'project' each data point into a space in which they are linearly separable and find the classifier in that space. \n",
    "\n",
    "Given data $x_1,...x_N$, where $x_i \\in \\mathbb{R}^n$, such that the data is not strictly linearly spearable, our goal is to find a mapping $\\phi: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k$ in which the data points are strictly separable. \n",
    "\n",
    "In the next section you will try to find a mapping that does this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to see the kernel SVM data\n",
    "\n",
    "plt.figure()\n",
    "plotData(data2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Choosing a Kernel \n",
    "\n",
    "Given the data above, design a mapping $\\phi: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k$, that takes a data point $x=[x_1,x_2]^T \\in \\mathbb{R}^2$ and projects it into $\\mathbb{R}$, where it is linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotKernelData(phi,data2)\n",
    "\n",
    "print('Your transformed data looks like:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform SVM on your kernelized data. Note that data is now in $\\mathbb{R}$, so you may have to go back and adjust your code so that it works in any dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to perform SVM on the projected data using your function phi above\n",
    "\n",
    "td=transformData(phi,data2)\n",
    "mu=optimize(f,g,td,labels)\n",
    "kernel_beta,kernel_beta_0=lagrange_to_classifier(mu,td,labels)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plot1DClassifier(kernel_beta,kernel_beta_0) \n",
    "plotKernelData(phi,data2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to see what the classifier you found looks like in the original space, \n",
    "#as well as the support vectors for the data\n",
    "\n",
    "support_vectors=findSupportVectors(np.round(mu,10),data)\n",
    "\n",
    "plt.figure()\n",
    "plotData(data2)\n",
    "plotKernelSVM(-1*kernel_beta_0/kernel_beta)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
